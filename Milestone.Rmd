---
title: "Predicting Your Next Word - Witchcraft or a Data Science Capstone Project (Milestone)?"
author: "Philip Mateescu"
date: "September 3, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
DATASET_LOCATION = "/Users/philip/Projects/coursera-datascience/capstone/final"
library(tm)
library(stringi)
library(stringr)
library(ggplot2)
library(dplyr)
source('multiplot.R')
```

## Summary

This purpose of this project is to create an application that will predict the next word in a sequence of words entered by a user. 

This milestone deals specifically with Exploratory Data Analysis using the Swiftkey dataset.  
 The dataset, available in four different languages: English, Finish, German, and Russian, contains text extracted from three different sources: blog, news posts, and twitter twits.

The purpose of this exploration is to understand the distribution of words, n-grams, frequency, and relationship between them.

## Loading and Cleaning the Data

For linguistical reasons, we will be performing the exploration on the English data sets.

```{r file_loading, cache=TRUE, warning=FALSE}
f_blogs <- file(paste(DATASET_LOCATION, "en_US", "en_US.blogs.txt", sep='/'), open="rb")
f_news <- file(paste(DATASET_LOCATION, "en_US", "en_US.news.txt", sep='/'), open="rb")
f_twitter <- file(paste(DATASET_LOCATION, "en_US", "en_US.twitter.txt", sep='/'), open="rb")

data_blogs <- readLines(f_blogs)
data_news <- readLines(f_news)
data_twitter <- readLines(f_twitter)

close(f_blogs); close(f_news); close(f_twitter)
rm(f_blogs, f_news, f_twitter)
```

Let's look at the dimensionality of the text:

```{r summary, cache=TRUE}
words_blogs = stri_count_words(data_blogs)
words_news = stri_count_words(data_news)
words_twitter = stri_count_words(data_twitter)
summary_frame <- data.frame(
    file = c('blogs', 'news', 'twitter'),
    entries = c(length(data_blogs), length(data_news), length(data_twitter)),
    total_words = c(sum(words_blogs), sum(words_news), sum(words_twitter)),
    mean_words_per_entry = c(mean(words_blogs), mean(words_news), mean(words_twitter)))

summary_frame
```

We see that even though we have almost 3 times as many twitter entries as we do blog posts,
the amount of words available for each source is about the same. 

Secondary, we notice that the number of words in the twitter posts, due to character length limitations, tends to be significantly smaller than posts and blogs. This could be an interesting factor in our predictions: whether you're writing long-form text or short-form messages.

## Exploratory Analysis

The size of the dataset makes exploratory analysis on the whole set slow and time consuming. As such, we will strip the dataset to a sample of roughly 1% of its original size:

```{r sample, cache=TRUE}
set.seed(123)
sample_size = 0.01
sample_blogs <- sample(data_blogs, length(data_blogs) * sample_size)
sample_news <- sample(data_news, length(data_news) * sample_size)
sample_twitter <- sample(data_twitter, length(data_twitter) * sample_size)
sample_all <- c(sample_blogs, sample_news, sample_twitter)
sample_longform <- c(sample_blogs, sample_news)
sample_shortform <- c(sample_twitter)
length(sample_all)
sum(stri_count_words(sample_all))
rm(data_blogs, data_news, data_twitter)  # free up memory
gc()
summary(sample_shortform)
```

There are three steps to pre-processing the data for Natural Language Processing, NLP, analysis:

1. Construct a corpus from the data (using `VectorSource` and `Corpus` from the [tm package](https://cran.r-project.org/web/packages/tm/index.html))
2. Tokenize the corpus by removing special characters, numbers, etc.
3. Building a basic *n-gram* model.

*Note 1*: I have chosen to not remove profanity from the dataset as I view its presence a flag of academic interest and a mirror of current state of communication. After all, we want our predictions to mimic human usage, not to be moral linguistic arbitators.

*Note 2*: I have also chosen to not remove *stopwords*: *and, the, in, to*, etc. They are typically excluded from NLP analysis due to high-frequency/low informational volume, but in phrase construction, particularly in higher-level n-grams, there presence can significantly improve prediction accuracy.

```{r corpus, cache=TRUE}
preProcess <- function(x) {
    ret <- tm_map(x,  content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),  mc.cores=1)
    
    #Removing special characters and URLs
    # toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x,fixed=TRUE))
    #ret <- tm_map(ret, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
    #ret <- tm_map(ret, toSpace, "@[^\\s]+")
    
    ret <- tm_map(ret, content_transformer(tolower))
    ret <- tm_map(ret, stripWhitespace)
    # ret <- tm_map(ret, stemDocument)
    
    ret <- tm_map(ret, removePunctuation)
    ret <- tm_map(ret, removeNumbers)
    # do not remove stop-word because they're important in phrase construction
    # ret<-tm_map(ret, removeWords, stopwords('english'))
    return(ret)
}
corpus_all <- preProcess(VCorpus(VectorSource(sample_all)))
corpus_longform <- preProcess(VCorpus(VectorSource(sample_longform)))
corpus_shortform <- preProcess(VCorpus(VectorSource(sample_shortform)))
```

In NLP an *n-gram* is a sequence of *n* words that appear in the analyzed text. For the purpose of our analysis and later application, we will create *n-grams* of size 1 (unigrams), 2 (bigrams),  and 3 (trigrams).

```{r 1-gram, cached=TRUE}

# term matrices for all tokenizers
# use removeSparseTerms to remove very infrequent terms
dtm_all_1 <- removeSparseTerms(TermDocumentMatrix(corpus_all), 0.999)

# term matrices for long-form tokenizers
# dtm_longform_1 <- removeSparseTerms(TermDocumentMatrix(corpus_longform), 0.999)

# term matrices for short-form tokenizers
dtm_shortform_1 <- removeSparseTerms(TermDocumentMatrix(corpus_shortform), 0.999)
```

### Looking at data

Let's compare the frequency of single words in the *all*, short-form, and long-form corpora.

```{r frequency_function, warning=FALSE}
calc_frequency <- function(tdm) {
    f <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
    df <- data.frame(word=names(f), freq=f)
    return(df)
}
plot_freq <- function(tdm, title, xaxis.title="Words") {
    data <- calc_frequency(tdm)
    ggplot(data[1:20,], aes(reorder(word, -freq), freq)) +
        labs(x = xaxis.title, y="Frequency") +
        ggtitle(title) + 
        theme(axis.text.x = element_text(angle=90, size=10, hjust = 1)) +
        geom_bar(stat = 'identity')
}
```

```{r unigram_plots}
all_1 <- plot_freq(dtm_all_1, 'All Sources')
sf_1 <- plot_freq(dtm_shortform_1, 'Short-form Sources')
multiplot(all_1, sf_1, cols=2)
```

Some interesting change in frequency positions, aren't there?

Now let's explore the same for bi-gram, tri-gram, and quad-grams.

```{r n-grams, cache=TRUE, warning=FALSE}
# due to issues with RWeka due to Java, we're using tokenizer suggested in http://stackoverflow.com/questions/37817975/error-in-rweka-in-r-package

ngramTokenizer <- function(corpus, n) unlist(lapply(ngrams(words(corpus), n), paste, collapse = " "), use.names = FALSE)
BigramTokenizer <- function(corpus) ngramTokenizer(corpus, 2)
# BigramTokenizer <- function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
TrigramTokenizer <- function(corpus) ngramTokenizer(corpus, 3)

# all works
dtm_all_2 <- removeSparseTerms(TermDocumentMatrix(corpus_all, control=list(tokenize=BigramTokenizer)), 0.999)
dtm_all_3 <- removeSparseTerms(TermDocumentMatrix(corpus_all, control=list(tokenize=TrigramTokenizer)), 0.999)

# short-form posts
dtm_shortform_2 <- removeSparseTerms(TermDocumentMatrix(corpus_shortform, control=list(tokenize=BigramTokenizer)), 0.999)
dtm_shortform_3 <- removeSparseTerms(TermDocumentMatrix(corpus_shortform, control=list(tokenize=TrigramTokenizer)), 0.999)
```

#### Bi-gram comparison

```{r bi-gram}
all_2 <- plot_freq(dtm_all_2, 'All Sources', xaxis.title = "Bi-grams")
sf_2 <- plot_freq(dtm_shortform_2, 'Short-form Sources', xaxis.title = "Bi-grams")
multiplot(all_2, sf_2, cols=2)
```

#### Tri-gram comparison

```{r tri-gram}
all_3 <- plot_freq(dtm_all_3, 'All Sources', xaxis.title = "Tri-grams")
sf_3 <- plot_freq(dtm_shortform_3, 'Short-form Sources', xaxis.title = 'Tri-grams')
multiplot(all_3, sf_3, cols=2)
```

## Conclusion

The different sources, in particular along the short vs long-form content, result in different frequency of n-grams. This might be important if we were to target our suggestion engine based on the context of the user, e.g. use short-form engram NLP for text messages, but long-form for notes or journals. 

We notice that the vocabulary used in long-form writing tends to be "richer" than that use on Twitter, however, high frequency of words like *you*, *your*, or *this* is indicative of the direct and immediate communicative form of the social media platform.

Stopwords are very present, particularly for the higher level n-grams. This is not surprising because the larger the engram the more similar to typical gramatical construction the phrases become; it other words, large enough engrams will be almost entire sentences.

Finally, the dataset and *document-term matrices* take a very significant time to load and process. More consideration needs to be given in the next steps to possible optimizations.

## Next Steps

* Build and explore models without stopwords.
* Build and explore 4-gram models.
* Build a predictive algorithm using the Shinny Apps platform.
* Measure and improve the performance of the prediction algorithm to support running on platforms with more limited resources.
* Consider an iterative process/algorithm where we switch back and forth between bi-, tri-, and quad-grams based on the availability of the higher level engram (e.g. if there is no 4-gram available, switch back to the 3-gram) and it's probability (e.g. given 3 words, if a certain 2-level engram made from the last 2 words appears more frequent than any of the matching 4-grams, weigh the 2-gram more than the 4-grams).